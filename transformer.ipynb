{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choi-yongsuk/deep-learning-nlp/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSjVs8WLygwX"
      },
      "source": [
        "import os\n",
        "\n",
        "# 수학 관련 라이브러리\n",
        "import numpy as np\n",
        "import math\n",
        "# pytorch 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOFFox88ygwX"
      },
      "source": [
        "# sample input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXfXDvaRygwY"
      },
      "source": [
        "sample_input = torch.tensor([[  101,  2572,  3217,  5831,  5496,  2010,  2567,  1010,  3183,  2002,\n",
        "         2170,  1000,  1996,  7409,  1000,  1010,  1997,  9969,  4487, 23809,\n",
        "         3436,  2010,  3350,  1012,   102,  7727,  2000,  2032,  2004,  2069,\n",
        "         1000,  1996,  7409,  1000,  1010,  2572,  3217,  5831,  5496,  2010,\n",
        "         2567,  1997,  9969,  4487, 23809,  3436,  2010,  3350,  1012,   102,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0]])\n",
        "\n",
        "\n",
        "\n",
        "sample_label = torch.tensor([1])\n",
        "\n",
        "sample = [sample_input,sample_label]\n",
        "\n",
        "# label : 1 \n",
        "# sentence 1 : Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n",
        "# sentence 2 : Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
        "\n",
        "\n",
        "sample_config = {\n",
        "    \"dim\": 768,\n",
        "    \"dim_ff\": 3072,\n",
        "    \"n_layers\": 12,\n",
        "    \"p_drop_attn\": 0.1,\n",
        "    \"n_heads\": 12,\n",
        "    \"p_drop_hidden\": 0.1,\n",
        "    \"max_len\": 512,\n",
        "    \"n_segments\": 2,\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __getattr__(self, name):\n",
        "        return self[name]\n",
        "model_config = AttributeDict(sample_config)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmnf_7LYygwZ",
        "outputId": "5fac0bc8-6869-48e3-e06c-332da9da536d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_input.size()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiAuHp_aygwa"
      },
      "source": [
        "# Activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5iaNO2Qygwa"
      },
      "source": [
        "def gelu(x):\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b2rNAaQygwa"
      },
      "source": [
        "# Layer normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGIsARItygwb"
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, cfg, variance_epsilon=1e-12):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(cfg.dim))\n",
        "        self.beta  = nn.Parameter(torch.zeros(cfg.dim))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # get mean, variance\n",
        "        u = x.mean(-1, keepdim=True) # sequence 방향 mean\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True) # sequence 방향 variance\n",
        "        \n",
        "        # normalize\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # (x - mean)/std \n",
        "        \n",
        "        return self.gamma * x + self.beta # gamma, beta를 이용해 mean, std 조정"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq8hPlQ9ygwb"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWtg21x3ygwc"
      },
      "source": [
        "def get_sinusoid_encoding_table(n_position, d_model):\n",
        "    def cal_angle(position, hid_idx):\n",
        "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "    return torch.FloatTensor(sinusoid_table)\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"The embedding module from word, position and token_type embeddings.\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.dim) # token embedding\n",
        "        self.pos_embed = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(cfg.max_len, cfg.dim),freeze=True) # position embedding\n",
        "\n",
        "        self.norm = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long, device=x.device) # 0,1,2,3,4,5, ..., seq_len-1\n",
        "        pos = pos.unsqueeze(0).expand_as(x) # (S,) -> (B, S)\n",
        "\n",
        "        e = self.tok_embed(x) + self.pos_embed(pos)\n",
        "        return self.drop(self.norm(e))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzb7t4PBygwc"
      },
      "source": [
        "model = Embeddings(model_config)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRMUNZ1dygwd",
        "outputId": "8c1542f8-d962-457c-a1fe-ef2713ebb938",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "out = model(sample[0])\n",
        "out.size()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhT3QmbKygwd",
        "outputId": "a0e1dbce-e353-4d51-9e61-9f636d3c9429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "out"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5921,  0.5981, -0.0000,  ...,  0.8341, -0.2645,  0.4910],\n",
              "         [ 0.1814, -0.3599,  0.9116,  ...,  1.9829, -0.4326,  0.4979],\n",
              "         [-0.1754, -1.3611, -0.0797,  ...,  0.0000, -2.1423,  1.1560],\n",
              "         ...,\n",
              "         [ 0.3456,  2.0728, -0.6513,  ...,  1.2546, -0.3719,  1.4951],\n",
              "         [ 1.2529,  2.2094, -1.5495,  ...,  1.2462, -0.0000,  1.4851],\n",
              "         [ 1.8631,  1.5187, -2.0426,  ...,  0.0000, -0.3676,  1.4790]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJv4r1Ihygwe"
      },
      "source": [
        "#  Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CsAx5beygwe"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    #Scaled Dot Product Attention\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None, dropout=None):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "                 / math.sqrt(query.size(-1)) # scale\n",
        "        print(mask.size(), scores.size())\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        p_attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            p_attn = dropout(p_attn)\n",
        "\n",
        "        return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrF_s93Iygwe"
      },
      "source": [
        "def split_last(x, shape):\n",
        "    # [B,T,H] -> [B,T,H1,H2]\n",
        "    \"split the last dimension to given shape\"\n",
        "    shape = list(shape)\n",
        "    assert shape.count(-1) <= 1\n",
        "    if -1 in shape:\n",
        "        shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))\n",
        "    return x.view(*x.size()[:-1], *shape)\n",
        "\n",
        "def merge_last(x, n_dims):\n",
        "    \"merge the last n_dims to a dimension\"\n",
        "    s = x.size()\n",
        "    assert n_dims > 1 and n_dims < len(s)\n",
        "    return x.view(*s[:-n_dims], -1)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multi-Headed Dot Product Attention \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.proj_q = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_k = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.proj_v = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_attn)\n",
        "        self.scores = None # for visualization\n",
        "        self.n_heads = cfg.n_heads\n",
        "\n",
        "    def forward(self, x, mask, x_q=None):\n",
        "        \"\"\"\n",
        "        x, q(query), k(key), v(value) : (B(batch_size), S(seq_len), D(dim))\n",
        "        mask : (B(batch_size) x S(seq_len))\n",
        "        * split D(dim) into (H(n_heads), W(width of head)) ; D = H * W\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        \n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        if x_q is None:\n",
        "            q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)\n",
        "        else:\n",
        "            q, k, v = self.proj_q(x_q), self.proj_k(x), self.proj_v(x)\n",
        "        q, k, v = (split_last(x, (self.n_heads, -1)).transpose(1, 2)\n",
        "                   for x in [q, k, v])\n",
        "        # (B, H, S, W) @ (B, H, W, S) -> (B, H, S, S) -softmax-> (B, H, S, S)\n",
        "        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1)) # @ == torch.matmul (dot product)\n",
        "        if mask is not None:\n",
        "            mask = mask[:, None, :].float()\n",
        "            scores -= 10000.0 * (1.0 - mask)\n",
        "        scores = self.drop(F.softmax(scores, dim=-1))\n",
        "        # (B, H, S, S) @ (B, H, S, W) -> (B, H, S, W) -trans-> (B, S, H, W)\n",
        "        h = (scores @ v).transpose(1, 2).contiguous()\n",
        "        # -merge-> (B, S, D)\n",
        "        h = merge_last(h, 2)\n",
        "        self.scores = scores\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bR85zbXdygwf"
      },
      "source": [
        "# Base feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byqdScQoygwf"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\" FeedForward Neural Networks for each position \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg.dim, cfg.dim_ff)\n",
        "        self.fc2 = nn.Linear(cfg.dim_ff, cfg.dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, S, D) -> (B, S, D_ff) -> (B, S, D)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhffY-zTygwf"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1nz9UOcygwg"
      },
      "source": [
        "class Encoder_Block(nn.Module):\n",
        "    \"\"\" Transformer Block \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(cfg)\n",
        "        self.proj = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h = self.attn(x, mask)\n",
        "        h = self.norm1(x + self.drop(self.proj(h)))\n",
        "        h = self.norm2(h + self.drop(self.pwff(h)))\n",
        "        return h\n",
        "    \n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
        "    \n",
        "def get_attn_subsequent_mask(seq):\n",
        "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
        "    subsequent_mask = torch.tensor(subsequent_mask, device=seq.device).byte()\n",
        "    return subsequent_mask\n",
        "    \n",
        "    \n",
        "class Decoder_Block(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(cfg)\n",
        "        self.encoder_attention = MultiHeadAttention(cfg)\n",
        "        \n",
        "        self.norm1 = LayerNorm(cfg)\n",
        "        self.proj1 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        self.norm2 = LayerNorm(cfg)\n",
        "        self.proj2 = nn.Linear(cfg.dim, cfg.dim)\n",
        "        \n",
        "        self.pwff = PositionWiseFeedForward(cfg)\n",
        "        self.norm3 = LayerNorm(cfg)\n",
        "        \n",
        "        self.drop = nn.Dropout(cfg.p_drop_hidden)\n",
        "        \n",
        "    def forward(self,x , enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\n",
        "        \n",
        "        \n",
        "        # self-attention -> add&norm\n",
        "        h = self.self_attention(x, dec_self_attn_mask)\n",
        "        h = self.norm1(x + self.drop(self.proj1(h)))\n",
        "        \n",
        "        # encoder attention -> add&norm\n",
        "        h2 = self.encoder_attention(enc_outputs, dec_enc_attn_mask, x_q=h)\n",
        "        h = self.norm2(h + self.drop(self.proj2(h2))) \n",
        "        \n",
        "        # feedforward network\n",
        "        h = self.norm3(h + self.drop(self.pwff(h)))\n",
        "        \n",
        "        return h\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\" Transformer with Self-Attentive Blocks\"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        #====================encoder===========================\n",
        "        self.encoder_embed = Embeddings(cfg)\n",
        "        self.encoder_blocks = nn.ModuleList([Encoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "\n",
        "        #====================decoder============================\n",
        "        self.decoder_embed = Embeddings(cfg)\n",
        "        self.decoder_blocks = nn.ModuleList([Decoder_Block(cfg) for _ in range(cfg.n_layers)])\n",
        "        \n",
        "        #=========================================================\n",
        "        self.projection = nn.Linear(cfg.dim, cfg.vocab_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, enc_inputs, dec_inputs):\n",
        "        #============encoder============\n",
        "        h = self.encoder_embed(enc_inputs)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\n",
        "        for block in self.encoder_blocks:\n",
        "            h = block(h, enc_self_attn_mask)\n",
        "            \n",
        "        enc_outputs = h\n",
        "        \n",
        "        \n",
        "        #============decoder============\n",
        "        \n",
        "        # self attention mask\n",
        "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).float()\n",
        "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs).float()\n",
        "        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0)\n",
        "\n",
        "        # encoder attention mask\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\n",
        "        \n",
        "        \n",
        "        # embedding\n",
        "        h = self.decoder_embed(dec_inputs)\n",
        "        \n",
        "        \n",
        "        for block in self.decoder_blocks:\n",
        "            h = block(h, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
        "        #============projection==========\n",
        "        out = self.projection(h)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWK80vqMygwm",
        "outputId": "504ad715-e0e5-4175-a3f5-b087c1b14dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = Transformer(model_config)\n",
        "out = model(sample[0],sample[0])\n",
        "out.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 30522])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARGqrCFYygwr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}