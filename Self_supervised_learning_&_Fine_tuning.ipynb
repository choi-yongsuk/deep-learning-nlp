{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-supervised learning & Fine-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choi-yongsuk/deep-learning-nlp/blob/master/Self_supervised_learning_%26_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹¤ìŠµìë£Œ:\n",
        "# https://tinyurl.com/SKT20220818A"
      ],
      "metadata": {
        "id": "OgzwMfy_buzr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXpp2652LhYw"
      },
      "source": [
        "##**HuggingFace ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©í•˜ê¸°**\n",
        "ì§€ë‚œ ì‹œê°„ì—ëŠ” Huggingfaceì˜ Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ë²ˆ ì‹œê°„ì—ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ë“¤ì´ í¬í•¨ë˜ì–´ ìˆëŠ” Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ìì—°ì–´ì²˜ë¦¬ ë°ì´í„°ì…‹ë“¤ì„ ë°›ì„ ìˆ˜ ìˆëŠ” datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4yOyKBL_Cd"
      },
      "source": [
        "### **í•„ìš” íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ ë° import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ3ZXNISLUE8"
      },
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg0Htj5kMSyK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "# ë°ì´í„° í¬ë¡¤ë§ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Huggingface\n",
        "import transformers\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGqBFVTSMeGf"
      },
      "source": [
        "### **Huggingface's Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKAik1HiQuaL"
      },
      "source": [
        "- ì‹œì‘í•˜ê¸° ì „ì— Huggingfaceì—ì„œ ì œê³µí•˜ëŠ” Transformersì— ëŒ€í•˜ì—¬ ì•Œì•„ë´…ë‹ˆë‹¤. \n",
        "- ìì—°ì–´ ì²˜ë¦¬ ê´€ë ¨ ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ìˆì§€ë§Œ Transformerë¥¼ í™œìš©í•œ ìì—°ì–´ ì²˜ë¦¬ taskì—ì„œ ê°€ì¥ ë§ì´ í™œìš©ë˜ê³  ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” transformersì…ë‹ˆë‹¤.\n",
        "- pytorch versionì˜ BERTë¥¼ ê°€ì¥ ë¨¼ì € êµ¬í˜„í•˜ë©° ì£¼ëª©ë°›ì•˜ë˜ huggingfaceëŠ” í˜„ì¬ transformerê¸°ë°˜ì˜ ë‹¤ì–‘í•œ ëª¨ë¸ë“¤ì€ êµ¬í˜„ ë° ê³µê°œí•˜ë©° ë§ì€ ì£¼ëª©ì„ ë°›ê³  ìˆìŠµë‹ˆë‹¤.([Pre-trained Transformers](https://huggingface.co/models))\n",
        "- tensorflow, pytorch ë²„ì „ì˜ ëª¨ë¸ ëª¨ë‘ ê³µê°œë˜ì–´ ìˆì–´ ë‹¤ì–‘í•œ ìƒí™©ì—ì„œ í™œìš©í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤.\n",
        "- ë“±ë¡ëœ ëª¨ë¸ ì´ì™¸ì—ë„ custom modelì„ ì—…ë¡œë“œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- Transformers Documentationê³¼ ì‹¤ìŠµ ìë£Œë¥¼ ì´ìš©í•´ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.\n",
        "- [Transformers Library](https://huggingface.co/transformers/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzkIyBruVr0S"
      },
      "source": [
        "#### Main Classes\n",
        "- Configuration: https://huggingface.co/transformers/main_classes/configuration.html\n",
        "- AutoConfigì—ì„œëŠ” ë‹¤ì–‘í•œ ëª¨ë¸ì˜ configuration (í™˜ê²½ ì„¤ì •)ì„ string tagë¥¼ ì´ìš©í•´ ì‰½ê²Œ loadí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ê° Configì—ëŠ” í•´ë‹¹ ëª¨ë¸ architectureì™€ taskì— í•„ìš”í•œ ë‹¤ì–‘í•œ ì •ë³´(architecture ì¢…ë¥˜, ë ˆì´ì–´ ìˆ˜, hidden unit size, hyperparameter)ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "- [Pre-trained Transformers](https://huggingface.co/models)ì—ì„œ í•´ë‹¹ ëª¨ë¸ë“¤ì˜ name tagë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkqZkuOKXcXY"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "# AutoConfigì„ ì´ìš©í•´ \"BERT\" ëª¨ë¸ì˜ configurationì„ ë°›ì•„ë´…ì‹œë‹¤.\n",
        "config = AutoConfig.from_pretrained('bert-large-uncased')\n",
        "config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Ao6iCmxr3Y"
      },
      "source": [
        "# AutoConfigì„ ì´ìš©í•´ \"GPT-2\" ëª¨ë¸ì˜ configurationì„ ë°›ì•„ë´…ì‹œë‹¤.\n",
        "gpt_config = AutoConfig.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lffmrzsyOIx"
      },
      "source": [
        "gpt_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3h6XvCmY94O"
      },
      "source": [
        "print(config.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeAIs74NXsGn"
      },
      "source": [
        "# config ì„ í¸í•˜ê²Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ .to_dict() í•¨ìˆ˜ë¥¼ í†µí•´ dictí˜•ì‹ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "config_dict = config.to_dict()\n",
        "config_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbhKTouQnW-H"
      },
      "source": [
        "# AutoConfigì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , íŠ¹ì • ëª¨ë¸ì˜ configì„ì„ ëª…ì‹œí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "from transformers import BertConfig\n",
        "bertconfig = BertConfig.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ë ‡ê²Œ ëª…ì‹œí•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, ë‹¤ë¥¸ ëª¨ë¸ì˜ configì— ìˆëŠ” ê°’ë“¤ì„ ì›í•˜ëŠ” ëª¨ë¸ì˜ config í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í• ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "bert_in_gpt2_config = BertConfig.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "krdKFomAJ0mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_in_gpt2_config"
      ],
      "metadata": {
        "id": "tCc87jsCaUzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebVxNbPrdWhM"
      },
      "source": [
        "- Model: https://github.com/huggingface/transformers/tree/master/src/transformers/models\n",
        "    - Transformersì—ì„œëŠ” transformerê¸°ë°˜ì˜ ëª¨ë¸ architectureë¥¼ êµ¬í˜„í•´ë‘ì—ˆìŠµë‹ˆë‹¤.\n",
        "    - ìµœê·¼ì—ëŠ” [ViT](https://arxiv.org/abs/2010.11929)ì™€ ê°™ì´ Vision taskì—ì„œ í™œìš©í•˜ëŠ” transformer ëª¨ë¸ë“¤ì„ ì¶”ê°€í•˜ë©° ê·¸ í™•ì¥ì„±ì„ ë”í•´ê°€ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "    - ëª¨ë¸ architecture ë¿ë§Œ ì•„ë‹ˆë¼ ê´€ë ¨ taskì— ì ìš©ê°€ëŠ¥í•œ í˜•íƒœì˜ êµ¬í˜„ì²´ë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n",
        "    - BERT êµ¬í˜„ì²´ì—ì„œ ì œê³µí•˜ê³  ìˆëŠ” classë¥¼ í™•ì¸í•˜ê³  í•´ë‹¹ êµ¬ì¡°ë¥¼ ì´ìš©í•´ í•™ìŠµí•œ ëª¨ë¸ë“¤ì„ loadí•´ë³´ê² ìŠµë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSZY2ir-dVg_"
      },
      "source": [
        "from transformers import BertForMaskedLM, BertForQuestionAnswering, BertForSequenceClassification, BertForTokenClassification, BertForMultipleChoice, BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTd3SURV-_sY"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT model\n",
        "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb3a7WV%2FbtqVZTeLXwY%2FmDrKNb2oGLzUJPW5N7Azlk%2Fimg.png)"
      ],
      "metadata": {
        "id": "D4u3DrKdwYks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT ëª¨ë¸ì€ transformer ëª¨ë¸ì˜ \"Encoder\" ë¶€ë¶„ë§Œ ì‚¬ìš©í•œ í˜•íƒœì˜ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "![](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)"
      ],
      "metadata": {
        "id": "vlzWzmIpw-O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface libraryë¥¼ ì´ìš©í•´ ì§ì ‘ BERT ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "4KXTtJvIxMeM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YosK7I-P_GSp"
      },
      "source": [
        "bertmodel = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT ëª¨ë¸ì˜ ë ˆì´ì–´ë“¤ì„ í™•ì¸í•´ë³´ì‹œë©´, ì•„ë˜ì™€ ê°™ì€ ë ˆì´ì–´ë“¤ë¡œ êµ¬ì„±ëœ ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "\n",
        "- word_embeddings, position_mebeddings (Transformerì˜ word embeddingê³¼ positionl encoding)\n",
        "- token_type_embeddings (BERTì— ìƒˆë¡­ê²Œ ì¶”ê°€ëœ ì…ë ¥ ë¬¸ì¥ì˜ ì¸ë±ìŠ¤ ì„ë² ë”©)\n",
        "- BertLayer\n",
        "  - attention (multi-head attention)\n",
        "  - intermediate + output (FeedFoward)\n",
        "\n",
        "ì´ëŠ” Transformer encoderì— token type embeddingë§Œ ì¶”ê°€ëœ í˜•íƒœì…ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "15WI1FeSxYm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel"
      ],
      "metadata": {
        "id": "pNvNJ1YkxVi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë¸ì˜ word embeddingì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ”, ìš°ì„  ì…ë ¥ ë¬¸ì¥ì„ word (í˜¹ì€ sub-word)ë“¤ë¡œ ë‚˜ëˆ„ì–´ indexë¡œ ë³€í™˜í•´ì£¼ëŠ” ì‘ì—…ì´ í•„ìš”í•˜ê² ì£ ? ìš°ë¦¬ëŠ” ì´ëŸ° ì¼ì„ í•´ì£¼ëŠ” ê²ƒì„ tokenizerë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.\n",
        "\n",
        "BERT ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” tokenizerë„ ë¶ˆëŸ¬ì™€ë´…ì‹œë‹¤.\n",
        "\n",
        "BERT tokenizerëŠ” ë¬¸ì¥ì´ ì…ë ¥ë˜ë©´, \n",
        "- ì—¬ëŸ¬ê°œì˜ tokenë“¤ë¡œ ìª¼ê°œì£¼ê³ ,\n",
        "- ìª¼ê°œì§„ tokenë“¤ì„ indexë¡œ ë³€í™˜í•´ì£¼ê³ ,\n",
        "- attentionì— ì‚¬ìš©ë˜ëŠ” mask, token type(ë¬¸ì¥ ì¸ë±ìŠ¤), ì¶”ê°€ë¡œ í•„ìš”í•œ í† í°ë“¤ (CLS, SEP)ê¹Œì§€ ì•Œì•„ì„œ ì¶”ê°€í•´ì¤ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "NaeTjZaiyTSj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRq_o20Z_RWO"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBaCebJg_QEV"
      },
      "source": [
        "input = tokenizer('Hello, world. The dog is so cute. I love you.')\n",
        "input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "íŠ¹ì • íƒœìŠ¤í¬ì— íŠ¹í™”ëœ í˜•íƒœì˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- BertForQuestionAnsweringì˜ ê²½ìš° ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ output dimensionì´ 2ì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ë‹¨ìˆœíˆ \"bert-base-uncased\"ë¥¼ ì´ìš©í•´ ë¶ˆëŸ¬ì˜¤ë©´, BERT pre-trainingì´ ì™„ë£Œëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ë§Œ, \"deepset/bert-base-cased-squad2\"ì—ì„œ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ë©´ SQuAD 2.0 ë°ì´í„°ì…‹ì— fine-tuning ê¹Œì§€ ì™„ë£Œëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Ppba3IlBIOoj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xUTu1rOgEF4"
      },
      "source": [
        "bert_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa"
      ],
      "metadata": {
        "id": "_M5KP1UQYK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr15PAUR_3Dg"
      },
      "source": [
        "bert_qa = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2') # SQuAD 2.0 ë°ì´í„°ì…‹ì— fine-tuningê¹Œì§€ ì™„ë£Œëœ ëª¨ë¸ì…ë‹ˆë‹¤."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.keys()"
      ],
      "metadata": {
        "id": "ceQ0krREWVPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer('Hello, world. The dog is so cute. I love you.', return_tensors='pt')\n",
        "\n",
        "input['input_ids'], input['input_ids'].shape"
      ],
      "metadata": {
        "id": "9D-9AWHpYdvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa(**input)['start_logits']"
      ],
      "metadata": {
        "id": "-EgTx1p0WYVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa(**input)['start_logits'].shape"
      ],
      "metadata": {
        "id": "Ysy87vTwWls-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Classification ëª¨ë¸ì˜ ê²½ìš°, ì…ë ¥ëœ ê° ë‹¨ì–´ë§ˆë‹¤ì˜ classification ì´ í•„ìš”í•œ ê²½ìš°ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. (ex. Named Entity Recognition)"
      ],
      "metadata": {
        "id": "kGacijOQI9Tc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laz4aEergyll"
      },
      "source": [
        "bert_token_cls = BertForTokenClassification.from_pretrained('ckiplab/bert-base-chinese-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer('Hello, world. The dog is so cute. I love you.', return_tensors='pt')\n",
        "bert_token_cls(**input)"
      ],
      "metadata": {
        "id": "lKpTY9I9W8vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnOb20vWjCfL"
      },
      "source": [
        "# Optimization\n",
        "https://huggingface.co/transformers/main_classes/optimizer_schedules.html\n",
        "- optimizationì—ì„œëŠ” ë„ë¦¬ ì“°ì´ê³  ìˆëŠ” ë‹¤ì–‘í•œ optimizerë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "- ì´ì™€ ê´€ë ¨ëœ learning rateì„ ì¡°ì ˆí•˜ëŠ” schedulerë„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "- ë¬¼ë¡ , PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” ê²ƒì„ ì‚¬ìš©í•´ë„ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TMah6G-jFxx"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrJ-1j8sjL7L"
      },
      "source": [
        "bert_maskedlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "parameters = bert_maskedlm.parameters()\n",
        "# parameters = bert_maskedlm.named_parameters()\n",
        "optimizer = AdamW(parameters, lr=5e-5)\n",
        "total_training_step = 100\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_training_step/10), num_training_steps=total_training_step)\n",
        "\n",
        "# loss.backward()\n",
        "optimizer.step()\n",
        "scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3y5KeSBnk1B"
      },
      "source": [
        "# Pipelines\n",
        "https://huggingface.co/transformers/main_classes/pipelines.html\n",
        "- pipelineì—ì„œëŠ” ë‹¨ 1ì¤„ë¡œ ëª¨ë¸ì„ loadí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- í˜„ì¬ pipelineì—ì„œ ì œê³µí•˜ê³  ìˆëŠ” taskë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "    - fill-mask\n",
        "    - text-classification\n",
        "    - text-generation\n",
        "    - sentiment-analysis\n",
        "    - text2text-generation\n",
        "    - ner\n",
        "    - translation_xx_to_yy\n",
        "    - zero-shot-classification\n",
        "- ì˜ˆì‹œë¥¼ í†µí•´ pipeline classë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqHLCdLRn2IO"
      },
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b3C4mx2RTot"
      },
      "source": [
        "fill_masker = pipeline(\"fill-mask\")\n",
        "fill_masker(\"I <mask> you.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC6hAIIsoaY3"
      },
      "source": [
        "classifier = pipeline(\"text-classification\")\n",
        "classifier(\"This restaurant is not bad.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTM5ecJ5oo4I"
      },
      "source": [
        "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
        "en_fr_translator(\"How old are you?\")\n",
        "# Quel Ã¢ge avez-vous?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_fr_translator(\"What is your name?\")"
      ],
      "metadata": {
        "id": "dIY15ttVc4g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2HC0J45qK5F"
      },
      "source": [
        "# Tokenizer\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html\n",
        "- tokenizerì—ì„œëŠ” tokenizationê³¼ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "- stringì„ tokenization, tokenì„ stringìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” ê¸°ëŠ¥ì€ input embeddingì„ ë§Œë“¤ê±°ë‚˜ modelì˜ outputì„ decodingí•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "- tokenizerì—ì„œëŠ” ì£¼ì–´ì§„ tokenization configë¥¼ ë°”íƒ•ìœ¼ë¡œ transformer inputìœ¼ë¡œ í•„ìš”í•œ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "Q. Pretrained modelë§Œì´ ì•„ë‹ˆë¼, tokenizerë„ pretrained ëœ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ê°€ ë¬´ì—‡ì¼ê¹Œìš”?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ttAihGCqN4t"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmTxNbAEB9Hk"
      },
      "source": [
        "tokenizer.get_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTs4AE_8qUXD"
      },
      "source": [
        "print(tokenizer.tokenize(\"I love natural language processing\"))\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I love natural language processing\")))\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.tokenize(\"I love natural language processing\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLk2J8TMzu1V"
      },
      "source": [
        "tokenizer(\"I love natural language processing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERTë¥¼ ì´ìš©í•œ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° sentiment classification\n",
        "\n",
        "BERTëª¨ë¸ì„ fine-tuningí•˜ì—¬ ë„¤ì´ë²„ ì˜í™”ë¦¬ë·°ê°€ ê¸ì •ì ì¸ ë¦¬ë·°ì¸ì§€, ì•„ë‹Œì§€ë¥¼ ë¶„ì„í•˜ëŠ” ì‹¤í—˜ì„ ì§„í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ì‹¤ì œ ìƒí™©ê³¼ ë¹„ìŠ·í•˜ê²Œ, ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "iko4cUPOW-Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 page ë°ì´í„°ë¥¼ ì½ì–´ì™€ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "page = 1\n",
        "url = f'https://movie.naver.com/movie/point/af/list.naver?&page={page}'\n",
        "url\n"
      ],
      "metadata": {
        "id": "XAbRwd9ffR-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# urlë¡œë¶€í„° htmlì •ë³´ë¥¼ ì–»ì–´ì˜µë‹ˆë‹¤.\n",
        "html = requests.get(url)\n",
        "#htmlì„ ë°›ì•„ì˜¨ ë¬¸ì„œë¥¼ .contentë¡œ ì§€ì • í›„ soupê°ì²´ë¡œ ë³€í™˜\n",
        "soup = BeautifulSoup(html.content,'html.parser')\n",
        "soup"
      ],
      "metadata": {
        "id": "vwZibuPCfTx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find_all : ì§€ì •í•œ íƒœê·¸ì˜ ë‚´ìš©ì„ ëª¨ë‘ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
        "reviews = soup.find_all(\"td\",{\"class\":\"title\"})\n",
        "reviews"
      ],
      "metadata": {
        "id": "AjpXuHMbfU94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data = []\n",
        "#í•œ í˜ì´ì§€ì˜ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ë·°ë¥¼ í•˜ë‚˜ì”© ë³´ë©´ì„œ ë°ì´í„° ì¶”ì¶œ\n",
        "for review in reviews:\n",
        "    # ì˜ˆì‹œ: <a class=\"report\" href=\"#\" onclick=\"report('nimo****', 'VYvWCsRvbPwqbJZjcJx6mHaPm5IRnI1v1MYlRfqlIlA=', 'ì¬ë°ŒëŠ”ë° ê²Ÿì•„ì›ƒ ë³´ë‹¨â€¦', '18375693', 'point_after');\" style=\"color:#8F8F8F\" title=\"ìƒˆ ì°½\">ì‹ ê³ </a>\n",
        "    sentence = review.find(\"a\",{\"class\":\"report\"}).get(\"onclick\").split(\"', '\")[2]\n",
        "    #ë§Œì•½ ë¦¬ë·° ë‚´ìš©ì´ ë¹„ì–´ìˆë‹¤ë©´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "    if sentence != \"\":\n",
        "        # ì˜ˆì‹œ: <a class=\"movie color_b\" href=\"?st=mcode&amp;sword=194196&amp;target=after\">í•œì‚°: ìš©ì˜ ì¶œí˜„</a>\n",
        "        movie = review.find(\"a\",{\"class\":\"movie color_b\"}).get_text()\n",
        "        # ì˜ˆì‹œ:  <span class=\"st_off\"><span class=\"st_on\" style=\"width:100%\">ë³„ì  - ì´ 10ì  ì¤‘</span></span><em>10</em>\n",
        "        score = review.find(\"em\").get_text()\n",
        "        review_data.append([movie,sentence,int(score)])\n",
        "\n",
        "review_data"
      ],
      "metadata": {
        "id": "bAAnHhPjfW--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_reviews(page):\n",
        "    review_data = []\n",
        "    url = f'https://movie.naver.com/movie/point/af/list.naver?&page={page}'\n",
        "    #get : requestë¡œ urlì˜  htmlë¬¸ì„œì˜ ë‚´ìš© ìš”ì²­\n",
        "    html = requests.get(url)\n",
        "    #htmlì„ ë°›ì•„ì˜¨ ë¬¸ì„œë¥¼ .contentë¡œ ì§€ì • í›„ soupê°ì²´ë¡œ ë³€í™˜\n",
        "    soup = BeautifulSoup(html.content,'html.parser')\n",
        "    #find_all : ì§€ì •í•œ íƒœê·¸ì˜ ë‚´ìš©ì„ ëª¨ë‘ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
        "    reviews = soup.find_all(\"td\",{\"class\":\"title\"})\n",
        "    \n",
        "    #í•œ í˜ì´ì§€ì˜ ë¦¬ë·° ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ë·°ë¥¼ í•˜ë‚˜ì”© ë³´ë©´ì„œ ë°ì´í„° ì¶”ì¶œ\n",
        "    for review in reviews:\n",
        "        sentence = review.find(\"a\",{\"class\":\"report\"}).get(\"onclick\").split(\"', '\")[2]\n",
        "        #ë§Œì•½ ë¦¬ë·° ë‚´ìš©ì´ ë¹„ì–´ìˆë‹¤ë©´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ\n",
        "        if sentence != \"\":\n",
        "            movie = review.find(\"a\",{\"class\":\"movie color_b\"}).get_text()\n",
        "            score = review.find(\"em\").get_text()\n",
        "            review_data.append([movie,sentence,int(score)])\n",
        "    return review_data\n",
        "\n",
        "\n",
        "from tqdm.contrib.concurrent import process_map # ë³‘ë ¬ì²˜ë¦¬+ì§„í–‰ë„ ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
        "review_data = process_map(get_page_reviews, range(1, 1000), max_workers=8, chunksize=8) # multiprocessingì„ í†µí•´ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "1Gs-ra-TfX21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data[:10]"
      ],
      "metadata": {
        "id": "iPLFnWxPk6gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(sum(review_data, []))\n",
        "df.columns = ['movie','review','score']\n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "lyfuR0XMfQ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VKYov-xRPGk"
      },
      "source": [
        "### **Training Movie Review Classifier with BERTForSequenceClassification Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywyxndAdRVlh"
      },
      "source": [
        "Pre-trained BERTì˜ config, tokenizer, modelì„ ê°ê° ë¶ˆëŸ¬ì˜¤ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë•Œ, í•œêµ­ì–´ì— íŠ¹í™”ë˜ì–´ í•™ìŠµëœ ëª¨ë¸ ì¤‘ kcBERTë¥¼ ì´ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "- kcBERT ëª¨ë¸ì€ ë„¤ì´ë²„ ë‰´ìŠ¤ì˜ ëŒ“ê¸€ê³¼ ëŒ€ëŒ“ê¸€ì„ ì´ìš©í•˜ì—¬ ì‚¬ì „í•™ìŠµí•œ ëª¨ë¸ë¡œ, ê¸°ì¡´ì˜ koBERT ëª¨ë¸ë“¤ì´ ì •í˜•í™”ëœ ì–¸ì–´ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì— ë¹„í•´ êµ¬ì–´ì²´ì™€ ì‹ ì¡°ì–´ ë“±ì„ ë” ë§ì´ í•™ìŠµí•œ ëª¨ë¸ì…ë‹ˆë‹¤. ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ì—­ì‹œ êµ¬ì–´ì²´ì™€ ì‹ ì¡°ì–´ê°€ ë§ì€ ë°ì´í„°ì´ë¯€ë¡œ, kcBERTê°€ ì í•©í•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "- https://github.com/Beomi/KcBERT\n",
        "\n",
        "- https://huggingface.co/beomi/kcbert-base\n",
        "<!-- https://huggingface.co/monologg/kobert -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaupmgDWMbs5"
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSCqAU4-SCkk"
      },
      "source": [
        "config = AutoConfig.from_pretrained('beomi/kcbert-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Wdh8VQ7iX9"
      },
      "source": [
        "config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNinIDjPrWOR"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzWtnViurXO5"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "# ì„±ëŠ¥í‰ê°€ì§€í‘œ \n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "  \n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "metadata": {
        "id": "_nDMxydLf74n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaverReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.x = data['review'].values\n",
        "        self.y = (data['score']>=7).astype(int).values\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenizer(self.x[idx], truncation=True)\n",
        "        item['labels'] = [self.y[idx]]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "\n",
        "train_dataset = NaverReviewDataset(df.iloc[:len(df)//5*4], tokenizer)\n",
        "test_dataset  = NaverReviewDataset(df.iloc[len(df)//5*4:], tokenizer)\n",
        "\n",
        "# train_dataset = NaverReviewDataset(df.iloc[:9000], tokenizer)\n",
        "# test_dataset  = NaverReviewDataset(df.iloc[9000:], tokenizer)"
      ],
      "metadata": {
        "id": "tKH_pdiFkZ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer,  DataCollatorWithPadding\n",
        " \n",
        "training_args = TrainingArguments(\n",
        "   output_dir=\"finetuning-sentiment\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=5,\n",
        "   per_device_eval_batch_size=5,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train_dataset,\n",
        "   eval_dataset=test_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "jB7LE82sf70_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jA7akNBqf7yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "e0nbUXIMf7sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTë¥¼ ì´ìš©í•œ NER\n",
        "\n",
        "https://huggingface.co/dslim/bert-base-NER"
      ],
      "metadata": {
        "id": "fiv2WA5OlMYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "P1iY64qqw30c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n"
      ],
      "metadata": {
        "id": "bOaEj6lplNnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°„ë‹¨í•œ í˜•íƒœì˜ inference ì˜ˆì œ\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"My name is Wolfgang and I live in Berlin\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "id": "XZuj-hAcf7lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(dataset['train']['ner_tags']).min().min()"
      ],
      "metadata": {
        "id": "OWTdjEoWxi03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ì˜ˆì œ - í•œêµ­ì–´ NERì„ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "model = AutoModelForTokenClassification.from_pretrained('beomi/kcbert-base')\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 7) # 0~6ê¹Œì§€\n",
        "model.num_labels = 7\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"kor_ner\")\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "KPE9iwiLf7ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ ì˜ˆì œ\n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "01bNRMg1pTAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "FOrBkVIhoXwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(data['tokens'])\n",
        "    labels = data['ner_tags']\n",
        "    if len(input_ids)>tokenizer.model_max_length:\n",
        "        input_ids = input_ids[:tokenizer.model_max_length]\n",
        "        labels = labels[:tokenizer.model_max_length]\n",
        "    input_ids = input_ids + [0] * (tokenizer.model_max_length - len(labels))\n",
        "    labels = labels + [0] * (tokenizer.model_max_length - len(labels))\n",
        "    token_type_ids = [0] * len(input_ids)\n",
        "    attention_mask = [1] * len(data['tokens']) + [0] * (tokenizer.model_max_length - len(data['tokens']))\n",
        "    return {'input_ids':input_ids, 'token_type_ids':token_type_ids, 'attention_mask':attention_mask, 'labels':labels}\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "val_dataset = val_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "zVbU2RlUoT5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer,  DataCollatorForTokenClassification\n",
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=np.reshape(predictions, -1), references=np.reshape(labels,-1))\n",
        "\n",
        "\n",
        " \n",
        "training_args = TrainingArguments(\n",
        "   output_dir=\"finetuning-ner\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train_dataset,\n",
        "   eval_dataset=val_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "bQoj7avif7fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "LqnJKBpgf7by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "mby2p4t3f7YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD ë°ì´í„°ì…‹ì„ ì´ìš©í•œ QA ëª¨ë¸ ë§Œë“¤ê¸°"
      ],
      "metadata": {
        "id": "XVd8sCKW0Z9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"KETI-AIR/korquad\", 'v1.0')"
      ],
      "metadata": {
        "id": "sFen3FY2f61C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ë§ˆë‹¤ êµ¬ë¶„ëœ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "raw_datasets.keys()"
      ],
      "metadata": {
        "id": "QqKNRKhp0tf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = raw_datasets['train']\n",
        "val_dataset = raw_datasets['dev']"
      ],
      "metadata": {
        "id": "zMyiFzCJ0e-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "3kREtcXBMgmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "tokenizer = AutoTokenizer.from_pretrained('sangrimlee/bert-base-multilingual-cased-korquad')\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('sangrimlee/bert-base-multilingual-cased-korquad')"
      ],
      "metadata": {
        "id": "FFv9t-KQ03SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = train_dataset[0]['question']\n",
        "context = train_dataset[0]['context']\n",
        "tokenizer(question, context)"
      ],
      "metadata": {
        "id": "MYC8UEDU03Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples['question'],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "  \n",
        "\n",
        "    for i, offset in enumerate([offset_mapping]):\n",
        "        answer = answers\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    inputs['labels'] = answer\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "MxgLtKqX03Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "OC3KkDtC2V3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(train_dataset[0])"
      ],
      "metadata": {
        "id": "vw6T9JEB02c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_function)\n",
        "val_dataset = val_dataset.map(preprocess_function)"
      ],
      "metadata": {
        "id": "6UD53tVLKQUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lWOI-mY7TNqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "from datasets import load_metric\n",
        "accuracy = load_metric('accuracy')\n",
        "# ì„±ëŠ¥í‰ê°€ì§€í‘œ \n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    labels = np.concatenate(labels, -1).T\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions.reshape(-1), references=labels.reshape(-1))\n",
        "\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "X-U2KHYb02ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "HJKuczSE02YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì´ ì‹¤ìŠµì—ì„œ ë¶ˆëŸ¬ì˜¨ ëª¨ë¸ì€ ì´ë¯¸ korquad ë°ì´í„°ì…‹ì— fine-tuningì´ ì™„ë£Œëœ ëª¨ë¸ì…ë‹ˆë‹¤. í•™ìŠµ ì—†ì´ evaluationì„ í•´ë´…ì‹œë‹¤.\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('sangrimlee/bert-base-multilingual-cased-korquad')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset[:10],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "fSf_l2-r02Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPTë¥¼ ì´ìš©í•œ ë¬¸ì¥ ìƒì„±"
      ],
      "metadata": {
        "id": "_OuyyP6ZKZ7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipelineì„ ì´ìš©í•œ ë°©ì‹\n",
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "id": "EBWPtdei02TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Hello, I'm a language model,\", max_length=50, num_return_sequences=10)\n"
      ],
      "metadata": {
        "id": "uEu9j5DQTttc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipelineì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ì‹\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_ids = tokenizer.encode(\"Some text to encode\", return_tensors='pt')\n",
        "\n",
        "generated_text_samples = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,\n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=2, #2-gram ë™ì–´ ë°˜ë³µì„ í”¼í•¨\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=0.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "yoZjwlwoOZML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, beam in enumerate(generated_text_samples):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Wo4KnlkhT-uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•œêµ­ì–´ GPT\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
        "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")"
      ],
      "metadata": {
        "id": "8NRfmNKFLxhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                           max_length=128,\n",
        "                           repetition_penalty=2.0,\n",
        "                           pad_token_id=tokenizer.pad_token_id,\n",
        "                           eos_token_id=tokenizer.eos_token_id,\n",
        "                           bos_token_id=tokenizer.bos_token_id,\n",
        "                           use_cache=True)"
      ],
      "metadata": {
        "id": "qz1odjD9OqEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "K5uC-dczUChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## koBARTë¥¼ ì´ìš©í•œ ë¬¸ë‹¨ ìš”ì•½"
      ],
      "metadata": {
        "id": "qEKMm6mJKt4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization')\n",
        "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')"
      ],
      "metadata": {
        "id": "I_OEYftc02Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"ê³¼ê±°ë¥¼ ë– ì˜¬ë ¤ë³´ì. ë°©ì†¡ì„ ë³´ë˜ ìš°ë¦¬ì˜ ëª¨ìŠµì„. ë…ë³´ì ì¸ ë§¤ì²´ëŠ” TVì˜€ë‹¤. ì˜¨ ê°€ì¡±ì´ ë‘˜ëŸ¬ì•‰ì•„ TVë¥¼ ë´¤ë‹¤. ê°„í˜¹ ê°€ì¡±ë“¤ë¼ë¦¬ ë‰´ìŠ¤ì™€ ë“œë¼ë§ˆ, ì˜ˆëŠ¥ í”„ë¡œê·¸ë¨ì„ ë‘˜ëŸ¬ì‹¸ê³  ë¦¬ëª¨ì»¨ ìŸíƒˆì „ì´ ë²Œì–´ì§€ê¸°ë„  í–ˆë‹¤. ê°ì ì„ í˜¸í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ â€˜ë³¸ë°©â€™ìœ¼ë¡œ ë³´ê¸° ìœ„í•œ ì‹¸ì›€ì´ì—ˆë‹¤. TVê°€ í•œ ëŒ€ì¸ì§€ ë‘ ëŒ€ì¸ì§€ ì—¬ë¶€ë„ ê·¸ë˜ì„œ ì¤‘ìš”í–ˆë‹¤. ì§€ê¸ˆì€ ì–´ë–¤ê°€. â€˜ì•ˆë°©ê·¹ì¥â€™ì´ë¼ëŠ” ë§ì€ ì˜›ë§ì´ ëë‹¤. TVê°€ ì—†ëŠ” ì§‘ë„ ë§ë‹¤. ë¯¸ë””ì–´ì˜ í˜œ íƒì„ ëˆ„ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ëŠ˜ì–´ë‚¬ë‹¤. ê°ìì˜ ë°©ì—ì„œ ê°ìì˜ íœ´ëŒ€í°ìœ¼ë¡œ, ë…¸íŠ¸ë¶ìœ¼ë¡œ, íƒœë¸”ë¦¿ìœ¼ë¡œ ì½˜í…ì¸  ë¥¼ ì¦ê¸´ë‹¤.\"\n",
        "\n",
        "raw_input_ids = tokenizer.encode(text)\n",
        "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
        "\n",
        "summary_ids = model.generate(torch.tensor([input_ids]))\n",
        "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "-ARsF6wFUSfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ivHTZY6mUt19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}