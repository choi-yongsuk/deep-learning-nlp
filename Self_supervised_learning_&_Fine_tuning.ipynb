{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self-supervised learning & Fine-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choi-yongsuk/deep-learning-nlp/blob/master/Self_supervised_learning_%26_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 실습자료:\n",
        "# https://tinyurl.com/SKT20220818A"
      ],
      "metadata": {
        "id": "OgzwMfy_buzr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXpp2652LhYw"
      },
      "source": [
        "##**HuggingFace 라이브러리 사용하기**\n",
        "지난 시간에는 Huggingface의 Tokenizers 라이브러리를 사용해보았습니다. 이번 시간에는 사전학습 모델들이 포함되어 있는 Transformers 라이브러리와 자연어처리 데이터셋들을 받을 수 있는 datasets 라이브러리를 사용해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4yOyKBL_Cd"
      },
      "source": [
        "### **필요 패키지 다운로드 및 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ3ZXNISLUE8"
      },
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg0Htj5kMSyK"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "# 데이터 크롤링을 위한 라이브러리\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Huggingface\n",
        "import transformers\n",
        "import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGqBFVTSMeGf"
      },
      "source": [
        "### **Huggingface's Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKAik1HiQuaL"
      },
      "source": [
        "- 시작하기 전에 Huggingface에서 제공하는 Transformers에 대하여 알아봅니다. \n",
        "- 자연어 처리 관련 여러 라이브러리가 있지만 Transformer를 활용한 자연어 처리 task에서 가장 많이 활용되고 있는 라이브러리는 transformers입니다.\n",
        "- pytorch version의 BERT를 가장 먼저 구현하며 주목받았던 huggingface는 현재 transformer기반의 다양한 모델들은 구현 및 공개하며 많은 주목을 받고 있습니다.([Pre-trained Transformers](https://huggingface.co/models))\n",
        "- tensorflow, pytorch 버전의 모델 모두 공개되어 있어 다양한 상황에서 활용하기 좋습니다.\n",
        "- 등록된 모델 이외에도 custom model을 업로드하여 사용할 수 있습니다.\n",
        "- Transformers Documentation과 실습 자료를 이용해 transformers 라이브러리에 대해 알아봅니다.\n",
        "- [Transformers Library](https://huggingface.co/transformers/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzkIyBruVr0S"
      },
      "source": [
        "#### Main Classes\n",
        "- Configuration: https://huggingface.co/transformers/main_classes/configuration.html\n",
        "- AutoConfig에서는 다양한 모델의 configuration (환경 설정)을 string tag를 이용해 쉽게 load할 수 있습니다.\n",
        "- 각 Config에는 해당 모델 architecture와 task에 필요한 다양한 정보(architecture 종류, 레이어 수, hidden unit size, hyperparameter)를 담고 있습니다.\n",
        "- [Pre-trained Transformers](https://huggingface.co/models)에서 해당 모델들의 name tag를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkqZkuOKXcXY"
      },
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "# AutoConfig을 이용해 \"BERT\" 모델의 configuration을 받아봅시다.\n",
        "config = AutoConfig.from_pretrained('bert-large-uncased')\n",
        "config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Ao6iCmxr3Y"
      },
      "source": [
        "# AutoConfig을 이용해 \"GPT-2\" 모델의 configuration을 받아봅시다.\n",
        "gpt_config = AutoConfig.from_pretrained('gpt2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lffmrzsyOIx"
      },
      "source": [
        "gpt_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3h6XvCmY94O"
      },
      "source": [
        "print(config.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeAIs74NXsGn"
      },
      "source": [
        "# config 을 편하게 사용하기 위해 .to_dict() 함수를 통해 dict형식으로 바꿀 수 있습니다.\n",
        "config_dict = config.to_dict()\n",
        "config_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbhKTouQnW-H"
      },
      "source": [
        "# AutoConfig을 사용하지 않고, 특정 모델의 config임을 명시하여 사용할 수도 있습니다.\n",
        "\n",
        "from transformers import BertConfig\n",
        "bertconfig = BertConfig.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이렇게 명시하여 사용하는 경우, 다른 모델의 config에 있는 값들을 원하는 모델의 config 형식으로 변환하여 사용할수도 있습니다.\n",
        "bert_in_gpt2_config = BertConfig.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "krdKFomAJ0mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_in_gpt2_config"
      ],
      "metadata": {
        "id": "tCc87jsCaUzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebVxNbPrdWhM"
      },
      "source": [
        "- Model: https://github.com/huggingface/transformers/tree/master/src/transformers/models\n",
        "    - Transformers에서는 transformer기반의 모델 architecture를 구현해두었습니다.\n",
        "    - 최근에는 [ViT](https://arxiv.org/abs/2010.11929)와 같이 Vision task에서 활용하는 transformer 모델들을 추가하며 그 확장성을 더해가고 있습니다.\n",
        "    - 모델 architecture 뿐만 아니라 관련 task에 적용가능한 형태의 구현체들이 있습니다.\n",
        "    - BERT 구현체에서 제공하고 있는 class를 확인하고 해당 구조를 이용해 학습한 모델들을 load해보겠습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSZY2ir-dVg_"
      },
      "source": [
        "from transformers import BertForMaskedLM, BertForQuestionAnswering, BertForSequenceClassification, BertForTokenClassification, BertForMultipleChoice, BertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTd3SURV-_sY"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer, AutoConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT model\n",
        "![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb3a7WV%2FbtqVZTeLXwY%2FmDrKNb2oGLzUJPW5N7Azlk%2Fimg.png)"
      ],
      "metadata": {
        "id": "D4u3DrKdwYks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT 모델은 transformer 모델의 \"Encoder\" 부분만 사용한 형태의 모델입니다.\n",
        "![](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)"
      ],
      "metadata": {
        "id": "vlzWzmIpw-O9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface library를 이용해 직접 BERT 모델을 불러와보도록 하겠습니다."
      ],
      "metadata": {
        "id": "4KXTtJvIxMeM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YosK7I-P_GSp"
      },
      "source": [
        "bertmodel = AutoModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT 모델의 레이어들을 확인해보시면, 아래와 같은 레이어들로 구성된 것을 알 수 있습니다. \n",
        "\n",
        "- word_embeddings, position_mebeddings (Transformer의 word embedding과 positionl encoding)\n",
        "- token_type_embeddings (BERT에 새롭게 추가된 입력 문장의 인덱스 임베딩)\n",
        "- BertLayer\n",
        "  - attention (multi-head attention)\n",
        "  - intermediate + output (FeedFoward)\n",
        "\n",
        "이는 Transformer encoder에 token type embedding만 추가된 형태입니다.\n"
      ],
      "metadata": {
        "id": "15WI1FeSxYm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel"
      ],
      "metadata": {
        "id": "pNvNJ1YkxVi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 word embedding을 사용하기 위해서는, 우선 입력 문장을 word (혹은 sub-word)들로 나누어 index로 변환해주는 작업이 필요하겠죠? 우리는 이런 일을 해주는 것을 tokenizer라고 부릅니다.\n",
        "\n",
        "BERT 모델이 사용하는 tokenizer도 불러와봅시다.\n",
        "\n",
        "BERT tokenizer는 문장이 입력되면, \n",
        "- 여러개의 token들로 쪼개주고,\n",
        "- 쪼개진 token들을 index로 변환해주고,\n",
        "- attention에 사용되는 mask, token type(문장 인덱스), 추가로 필요한 토큰들 (CLS, SEP)까지 알아서 추가해줍니다."
      ],
      "metadata": {
        "id": "NaeTjZaiyTSj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRq_o20Z_RWO"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBaCebJg_QEV"
      },
      "source": [
        "input = tokenizer('Hello, world. The dog is so cute. I love you.')\n",
        "input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "특정 태스크에 특화된 형태의 모델을 불러올 수도 있습니다.\n",
        "\n",
        "- BertForQuestionAnswering의 경우 마지막 레이어의 output dimension이 2인 것을 확인할 수 있습니다.\n",
        "- 단순히 \"bert-base-uncased\"를 이용해 불러오면, BERT pre-training이 완료된 모델을 사용하지만, \"deepset/bert-base-cased-squad2\"에서 모델을 불러오면 SQuAD 2.0 데이터셋에 fine-tuning 까지 완료된 모델을 불러올수도 있습니다."
      ],
      "metadata": {
        "id": "Ppba3IlBIOoj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xUTu1rOgEF4"
      },
      "source": [
        "bert_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa"
      ],
      "metadata": {
        "id": "_M5KP1UQYK3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr15PAUR_3Dg"
      },
      "source": [
        "bert_qa = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2') # SQuAD 2.0 데이터셋에 fine-tuning까지 완료된 모델입니다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input.keys()"
      ],
      "metadata": {
        "id": "ceQ0krREWVPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer('Hello, world. The dog is so cute. I love you.', return_tensors='pt')\n",
        "\n",
        "input['input_ids'], input['input_ids'].shape"
      ],
      "metadata": {
        "id": "9D-9AWHpYdvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa(**input)['start_logits']"
      ],
      "metadata": {
        "id": "-EgTx1p0WYVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa(**input)['start_logits'].shape"
      ],
      "metadata": {
        "id": "Ysy87vTwWls-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Classification 모델의 경우, 입력된 각 단어마다의 classification 이 필요한 경우에 사용할 수 있는 모델입니다. (ex. Named Entity Recognition)"
      ],
      "metadata": {
        "id": "kGacijOQI9Tc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laz4aEergyll"
      },
      "source": [
        "bert_token_cls = BertForTokenClassification.from_pretrained('ckiplab/bert-base-chinese-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer('Hello, world. The dog is so cute. I love you.', return_tensors='pt')\n",
        "bert_token_cls(**input)"
      ],
      "metadata": {
        "id": "lKpTY9I9W8vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnOb20vWjCfL"
      },
      "source": [
        "# Optimization\n",
        "https://huggingface.co/transformers/main_classes/optimizer_schedules.html\n",
        "- optimization에서는 널리 쓰이고 있는 다양한 optimizer를 제공하고 있습니다.\n",
        "- 이와 관련된 learning rate을 조절하는 scheduler도 제공하고 있습니다.\n",
        "- 물론, PyTorch 라이브러리에서 제공하는 것을 사용해도 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TMah6G-jFxx"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrJ-1j8sjL7L"
      },
      "source": [
        "bert_maskedlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "parameters = bert_maskedlm.parameters()\n",
        "# parameters = bert_maskedlm.named_parameters()\n",
        "optimizer = AdamW(parameters, lr=5e-5)\n",
        "total_training_step = 100\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_training_step/10), num_training_steps=total_training_step)\n",
        "\n",
        "# loss.backward()\n",
        "optimizer.step()\n",
        "scheduler.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3y5KeSBnk1B"
      },
      "source": [
        "# Pipelines\n",
        "https://huggingface.co/transformers/main_classes/pipelines.html\n",
        "- pipeline에서는 단 1줄로 모델을 load할 수 있습니다.\n",
        "- 현재 pipeline에서 제공하고 있는 task들은 다음과 같습니다.\n",
        "    - fill-mask\n",
        "    - text-classification\n",
        "    - text-generation\n",
        "    - sentiment-analysis\n",
        "    - text2text-generation\n",
        "    - ner\n",
        "    - translation_xx_to_yy\n",
        "    - zero-shot-classification\n",
        "- 예시를 통해 pipeline class를 어떻게 활용하는지 알아보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqHLCdLRn2IO"
      },
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b3C4mx2RTot"
      },
      "source": [
        "fill_masker = pipeline(\"fill-mask\")\n",
        "fill_masker(\"I <mask> you.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC6hAIIsoaY3"
      },
      "source": [
        "classifier = pipeline(\"text-classification\")\n",
        "classifier(\"This restaurant is not bad.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTM5ecJ5oo4I"
      },
      "source": [
        "en_fr_translator = pipeline(\"translation_en_to_fr\")\n",
        "en_fr_translator(\"How old are you?\")\n",
        "# Quel âge avez-vous?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_fr_translator(\"What is your name?\")"
      ],
      "metadata": {
        "id": "dIY15ttVc4g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2HC0J45qK5F"
      },
      "source": [
        "# Tokenizer\n",
        "https://huggingface.co/transformers/main_classes/tokenizer.html\n",
        "- tokenizer에서는 tokenization과 관련된 다양한 기능을 제공하고 있습니다.\n",
        "- string을 tokenization, token을 string으로 바꿔주는 기능은 input embedding을 만들거나 model의 output을 decoding하기 위해 사용됩니다.\n",
        "- tokenizer에서는 주어진 tokenization config를 바탕으로 transformer input으로 필요한 정보를 생성합니다.\n",
        "\n",
        "Q. Pretrained model만이 아니라, tokenizer도 pretrained 된 것을 사용하는 이유가 무엇일까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ttAihGCqN4t"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmTxNbAEB9Hk"
      },
      "source": [
        "tokenizer.get_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTs4AE_8qUXD"
      },
      "source": [
        "print(tokenizer.tokenize(\"I love natural language processing\"))\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"I love natural language processing\")))\n",
        "print(tokenizer.convert_tokens_to_string(tokenizer.tokenize(\"I love natural language processing\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLk2J8TMzu1V"
      },
      "source": [
        "tokenizer(\"I love natural language processing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT를 이용한 네이버 영화 리뷰 sentiment classification\n",
        "\n",
        "BERT모델을 fine-tuning하여 네이버 영화리뷰가 긍정적인 리뷰인지, 아닌지를 분석하는 실험을 진행해보겠습니다.\n",
        "\n",
        "이번 실습에서는 실제 상황과 비슷하게, 네이버 영화리뷰 데이터를 크롤링하여 데이터를 생성해보겠습니다."
      ],
      "metadata": {
        "id": "iko4cUPOW-Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 page 데이터를 읽어와보겠습니다.\n",
        "page = 1\n",
        "url = f'https://movie.naver.com/movie/point/af/list.naver?&page={page}'\n",
        "url\n"
      ],
      "metadata": {
        "id": "XAbRwd9ffR-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# url로부터 html정보를 얻어옵니다.\n",
        "html = requests.get(url)\n",
        "#html을 받아온 문서를 .content로 지정 후 soup객체로 변환\n",
        "soup = BeautifulSoup(html.content,'html.parser')\n",
        "soup"
      ],
      "metadata": {
        "id": "vwZibuPCfTx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find_all : 지정한 태그의 내용을 모두 찾아 리스트로 반환\n",
        "reviews = soup.find_all(\"td\",{\"class\":\"title\"})\n",
        "reviews"
      ],
      "metadata": {
        "id": "AjpXuHMbfU94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data = []\n",
        "#한 페이지의 리뷰 리스트의 리뷰를 하나씩 보면서 데이터 추출\n",
        "for review in reviews:\n",
        "    # 예시: <a class=\"report\" href=\"#\" onclick=\"report('nimo****', 'VYvWCsRvbPwqbJZjcJx6mHaPm5IRnI1v1MYlRfqlIlA=', '재밌는데 겟아웃 보단…', '18375693', 'point_after');\" style=\"color:#8F8F8F\" title=\"새 창\">신고</a>\n",
        "    sentence = review.find(\"a\",{\"class\":\"report\"}).get(\"onclick\").split(\"', '\")[2]\n",
        "    #만약 리뷰 내용이 비어있다면 데이터를 사용하지 않음\n",
        "    if sentence != \"\":\n",
        "        # 예시: <a class=\"movie color_b\" href=\"?st=mcode&amp;sword=194196&amp;target=after\">한산: 용의 출현</a>\n",
        "        movie = review.find(\"a\",{\"class\":\"movie color_b\"}).get_text()\n",
        "        # 예시:  <span class=\"st_off\"><span class=\"st_on\" style=\"width:100%\">별점 - 총 10점 중</span></span><em>10</em>\n",
        "        score = review.find(\"em\").get_text()\n",
        "        review_data.append([movie,sentence,int(score)])\n",
        "\n",
        "review_data"
      ],
      "metadata": {
        "id": "bAAnHhPjfW--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_reviews(page):\n",
        "    review_data = []\n",
        "    url = f'https://movie.naver.com/movie/point/af/list.naver?&page={page}'\n",
        "    #get : request로 url의  html문서의 내용 요청\n",
        "    html = requests.get(url)\n",
        "    #html을 받아온 문서를 .content로 지정 후 soup객체로 변환\n",
        "    soup = BeautifulSoup(html.content,'html.parser')\n",
        "    #find_all : 지정한 태그의 내용을 모두 찾아 리스트로 반환\n",
        "    reviews = soup.find_all(\"td\",{\"class\":\"title\"})\n",
        "    \n",
        "    #한 페이지의 리뷰 리스트의 리뷰를 하나씩 보면서 데이터 추출\n",
        "    for review in reviews:\n",
        "        sentence = review.find(\"a\",{\"class\":\"report\"}).get(\"onclick\").split(\"', '\")[2]\n",
        "        #만약 리뷰 내용이 비어있다면 데이터를 사용하지 않음\n",
        "        if sentence != \"\":\n",
        "            movie = review.find(\"a\",{\"class\":\"movie color_b\"}).get_text()\n",
        "            score = review.find(\"em\").get_text()\n",
        "            review_data.append([movie,sentence,int(score)])\n",
        "    return review_data\n",
        "\n",
        "\n",
        "from tqdm.contrib.concurrent import process_map # 병렬처리+진행도 시각화를 위한 라이브러리입니다.\n",
        "review_data = process_map(get_page_reviews, range(1, 1000), max_workers=8, chunksize=8) # multiprocessing을 통해 데이터를 크롤링합니다."
      ],
      "metadata": {
        "id": "1Gs-ra-TfX21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_data[:10]"
      ],
      "metadata": {
        "id": "iPLFnWxPk6gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(sum(review_data, []))\n",
        "df.columns = ['movie','review','score']\n",
        "df = df.dropna()\n",
        "df"
      ],
      "metadata": {
        "id": "lyfuR0XMfQ6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VKYov-xRPGk"
      },
      "source": [
        "### **Training Movie Review Classifier with BERTForSequenceClassification Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywyxndAdRVlh"
      },
      "source": [
        "Pre-trained BERT의 config, tokenizer, model을 각각 불러오겠습니다.\n",
        "\n",
        "이 때, 한국어에 특화되어 학습된 모델 중 kcBERT를 이용해보겠습니다.\n",
        "- kcBERT 모델은 네이버 뉴스의 댓글과 대댓글을 이용하여 사전학습한 모델로, 기존의 koBERT 모델들이 정형화된 언어만 사용하는 것에 비해 구어체와 신조어 등을 더 많이 학습한 모델입니다. 네이버 영화 리뷰 역시 구어체와 신조어가 많은 데이터이므로, kcBERT가 적합한 모델입니다.\n",
        "\n",
        "- https://github.com/Beomi/KcBERT\n",
        "\n",
        "- https://huggingface.co/beomi/kcbert-base\n",
        "<!-- https://huggingface.co/monologg/kobert -->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaupmgDWMbs5"
      },
      "source": [
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSCqAU4-SCkk"
      },
      "source": [
        "config = AutoConfig.from_pretrained('beomi/kcbert-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"beomi/kcbert-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7Wdh8VQ7iX9"
      },
      "source": [
        "config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNinIDjPrWOR"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzWtnViurXO5"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "# 성능평가지표 \n",
        "def compute_metrics(eval_pred):\n",
        "   load_accuracy = load_metric(\"accuracy\")\n",
        "   load_f1 = load_metric(\"f1\")\n",
        "  \n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
        "   return {\"accuracy\": accuracy, \"f1\": f1}"
      ],
      "metadata": {
        "id": "_nDMxydLf74n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaverReviewDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.x = data['review'].values\n",
        "        self.y = (data['score']>=7).astype(int).values\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenizer(self.x[idx], truncation=True)\n",
        "        item['labels'] = [self.y[idx]]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "\n",
        "train_dataset = NaverReviewDataset(df.iloc[:len(df)//5*4], tokenizer)\n",
        "test_dataset  = NaverReviewDataset(df.iloc[len(df)//5*4:], tokenizer)\n",
        "\n",
        "# train_dataset = NaverReviewDataset(df.iloc[:9000], tokenizer)\n",
        "# test_dataset  = NaverReviewDataset(df.iloc[9000:], tokenizer)"
      ],
      "metadata": {
        "id": "tKH_pdiFkZ3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer,  DataCollatorWithPadding\n",
        " \n",
        "training_args = TrainingArguments(\n",
        "   output_dir=\"finetuning-sentiment\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=5,\n",
        "   per_device_eval_batch_size=5,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train_dataset,\n",
        "   eval_dataset=test_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "jB7LE82sf70_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "jA7akNBqf7yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "e0nbUXIMf7sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT를 이용한 NER\n",
        "\n",
        "https://huggingface.co/dslim/bert-base-NER"
      ],
      "metadata": {
        "id": "fiv2WA5OlMYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "P1iY64qqw30c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n"
      ],
      "metadata": {
        "id": "bOaEj6lplNnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 간단한 형태의 inference 예제\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"My name is Wolfgang and I live in Berlin\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)"
      ],
      "metadata": {
        "id": "XZuj-hAcf7lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(dataset['train']['ner_tags']).min().min()"
      ],
      "metadata": {
        "id": "OWTdjEoWxi03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 예제 - 한국어 NER을 시도해보겠습니다.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
        "model = AutoModelForTokenClassification.from_pretrained('beomi/kcbert-base')\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 7) # 0~6까지\n",
        "model.num_labels = 7\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"kor_ner\")\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "test_dataset = dataset['test']"
      ],
      "metadata": {
        "id": "KPE9iwiLf7ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 예제\n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "01bNRMg1pTAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "FOrBkVIhoXwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(data['tokens'])\n",
        "    labels = data['ner_tags']\n",
        "    if len(input_ids)>tokenizer.model_max_length:\n",
        "        input_ids = input_ids[:tokenizer.model_max_length]\n",
        "        labels = labels[:tokenizer.model_max_length]\n",
        "    input_ids = input_ids + [0] * (tokenizer.model_max_length - len(labels))\n",
        "    labels = labels + [0] * (tokenizer.model_max_length - len(labels))\n",
        "    token_type_ids = [0] * len(input_ids)\n",
        "    attention_mask = [1] * len(data['tokens']) + [0] * (tokenizer.model_max_length - len(data['tokens']))\n",
        "    return {'input_ids':input_ids, 'token_type_ids':token_type_ids, 'attention_mask':attention_mask, 'labels':labels}\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "val_dataset = val_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "zVbU2RlUoT5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer,  DataCollatorForTokenClassification\n",
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=np.reshape(predictions, -1), references=np.reshape(labels,-1))\n",
        "\n",
        "\n",
        " \n",
        "training_args = TrainingArguments(\n",
        "   output_dir=\"finetuning-ner\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=16,\n",
        "   per_device_eval_batch_size=16,\n",
        "   num_train_epochs=2,\n",
        "   weight_decay=0.01,\n",
        "   save_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=train_dataset,\n",
        "   eval_dataset=val_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "bQoj7avif7fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "LqnJKBpgf7by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(eval_dataset=test_dataset)"
      ],
      "metadata": {
        "id": "mby2p4t3f7YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KorQuAD 데이터셋을 이용한 QA 모델 만들기"
      ],
      "metadata": {
        "id": "XVd8sCKW0Z9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_datasets = load_dataset(\"KETI-AIR/korquad\", 'v1.0')"
      ],
      "metadata": {
        "id": "sFen3FY2f61C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋마다 구분된 이름이 다를 수 있습니다.\n",
        "raw_datasets.keys()"
      ],
      "metadata": {
        "id": "QqKNRKhp0tf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = raw_datasets['train']\n",
        "val_dataset = raw_datasets['dev']"
      ],
      "metadata": {
        "id": "zMyiFzCJ0e-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "3kREtcXBMgmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "tokenizer = AutoTokenizer.from_pretrained('sangrimlee/bert-base-multilingual-cased-korquad')\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('sangrimlee/bert-base-multilingual-cased-korquad')"
      ],
      "metadata": {
        "id": "FFv9t-KQ03SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = train_dataset[0]['question']\n",
        "context = train_dataset[0]['context']\n",
        "tokenizer(question, context)"
      ],
      "metadata": {
        "id": "MYC8UEDU03Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(\n",
        "        examples['question'],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "  \n",
        "\n",
        "    for i, offset in enumerate([offset_mapping]):\n",
        "        answer = answers\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    inputs['labels'] = answer\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "MxgLtKqX03Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "OC3KkDtC2V3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(train_dataset[0])"
      ],
      "metadata": {
        "id": "vw6T9JEB02c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_function)\n",
        "val_dataset = val_dataset.map(preprocess_function)"
      ],
      "metadata": {
        "id": "6UD53tVLKQUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lWOI-mY7TNqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DefaultDataCollator\n",
        "from datasets import load_metric\n",
        "accuracy = load_metric('accuracy')\n",
        "# 성능평가지표 \n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    labels = np.concatenate(labels, -1).T\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions.reshape(-1), references=labels.reshape(-1))\n",
        "\n",
        "\n",
        "data_collator = DefaultDataCollator()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "X-U2KHYb02ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "HJKuczSE02YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 실습에서 불러온 모델은 이미 korquad 데이터셋에 fine-tuning이 완료된 모델입니다. 학습 없이 evaluation을 해봅시다.\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('sangrimlee/bert-base-multilingual-cased-korquad')\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset[:10],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "fSf_l2-r02Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT를 이용한 문장 생성"
      ],
      "metadata": {
        "id": "_OuyyP6ZKZ7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline을 이용한 방식\n",
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')"
      ],
      "metadata": {
        "id": "EBWPtdei02TF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator(\"Hello, I'm a language model,\", max_length=50, num_return_sequences=10)\n"
      ],
      "metadata": {
        "id": "uEu9j5DQTttc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline을 사용하지 않는 방식\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "input_ids = tokenizer.encode(\"Some text to encode\", return_tensors='pt')\n",
        "\n",
        "generated_text_samples = model.generate(\n",
        "    input_ids,\n",
        "    max_length=150,\n",
        "    num_return_sequences=5,\n",
        "    no_repeat_ngram_size=2, #2-gram 동어 반복을 피함\n",
        "    repetition_penalty=1.5,\n",
        "    top_p=0.92,\n",
        "    temperature=0.85,\n",
        "    do_sample=True,\n",
        "    top_k=125,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "yoZjwlwoOZML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, beam in enumerate(generated_text_samples):\n",
        "    print(\"{}: {}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Wo4KnlkhT-uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 GPT\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
        "tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
      ],
      "metadata": {
        "id": "8NRfmNKFLxhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
        "text = '근육이 커지기 위해서는'\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "gen_ids = model.generate(input_ids,\n",
        "                           max_length=128,\n",
        "                           repetition_penalty=2.0,\n",
        "                           pad_token_id=tokenizer.pad_token_id,\n",
        "                           eos_token_id=tokenizer.eos_token_id,\n",
        "                           bos_token_id=tokenizer.bos_token_id,\n",
        "                           use_cache=True)"
      ],
      "metadata": {
        "id": "qz1odjD9OqEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated = tokenizer.decode(gen_ids[0])\n",
        "print(generated)"
      ],
      "metadata": {
        "id": "K5uC-dczUChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## koBART를 이용한 문단 요약"
      ],
      "metadata": {
        "id": "qEKMm6mJKt4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gogamza/kobart-summarization')\n",
        "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')"
      ],
      "metadata": {
        "id": "I_OEYftc02Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"과거를 떠올려보자. 방송을 보던 우리의 모습을. 독보적인 매체는 TV였다. 온 가족이 둘러앉아 TV를 봤다. 간혹 가족들끼리 뉴스와 드라마, 예능 프로그램을 둘러싸고 리모컨 쟁탈전이 벌어지기도  했다. 각자 선호하는 프로그램을 ‘본방’으로 보기 위한 싸움이었다. TV가 한 대인지 두 대인지 여부도 그래서 중요했다. 지금은 어떤가. ‘안방극장’이라는 말은 옛말이 됐다. TV가 없는 집도 많다. 미디어의 혜 택을 누릴 수 있는 방법은 늘어났다. 각자의 방에서 각자의 휴대폰으로, 노트북으로, 태블릿으로 콘텐츠 를 즐긴다.\"\n",
        "\n",
        "raw_input_ids = tokenizer.encode(text)\n",
        "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id]\n",
        "\n",
        "summary_ids = model.generate(torch.tensor([input_ids]))\n",
        "tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "-ARsF6wFUSfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ivHTZY6mUt19"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}