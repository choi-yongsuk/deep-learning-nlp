{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdZxDCSz32vJUBE3dThnd8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choi-yongsuk/deep-learning-nlp/blob/master/nlp-article.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5ktdExQYCZZ",
        "outputId": "48b64808-320c-4a01-a94b-76e2ed06ec3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] and [2] [[0.]]\n",
            "[1] and [3] [[0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#sentence = (\"오늘도 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다.\",\n",
        "#            \"오늘도 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다.\",\n",
        "#            \"폭염을 피해 놀러왔다가 갑작스런 비로 망연자실하고 있습니다.\")\n",
        "#            \"123.\")\n",
        "\n",
        "\n",
        "sentence = (\"오늘도 폭염이 이어졌는데요, 내일은 반가운 비 소식이 있습니다.\",\n",
        "            \"바나나 바나나 바나나 바나나 바나나.\",\n",
        "#            \"폭염을 피해 놀러왔다가 갑작스런 비로 망연자실하고 있습니다.\")\n",
        "            \"바나나 바나나 .\",\n",
        "            \"바나나 바나나 .\",\n",
        "            \"바나나 바나나 .\",            \n",
        "            \"바나나.\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sentence) # vectorization\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(\"[1] and [2]\", cosine_similarity(tfidf_matrix[0], tfidf_matrix[1]))\n",
        "print(\"[1] and [3]\", cosine_similarity(tfidf_matrix[0], tfidf_matrix[2]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array = [\"F\", \"D\", \"A\", \"C\", \"A\", \"C\", \"F\", \"B\", \"C\", \"E\", \"D\", \"C\", \"F\", \"A\", \"B\", \"E\", \"F\", \"E\"]\n",
        "my_dict = {'a': array, 'b': 2, 'c': 3, 'd': 2, 'e': 1}\n",
        "print(type(my_dict))\n",
        "print(my_dict['a'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MgvTE9-OX1j",
        "outputId": "d9230d0d-45ff-4ec7-b73c-dfd09586d56e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "['F', 'D', 'A', 'C', 'A', 'C', 'F', 'B', 'C', 'E', 'D', 'C', 'F', 'A', 'B', 'E', 'F', 'E']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "중복제거 코드"
      ],
      "metadata": {
        "id": "o05Ljhd2lO5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "array = [\"F\", \"D\", \"A\", \"C\", \"A\", \"C\", \"F\", \"B\", \"C\", \"E\", \"D\", \"C\", \"F\", \"A\", \"B\", \"E\", \"F\", \"E\"]\n",
        "\n",
        "result = reduce(lambda acc, cur: acc if cur in acc else acc+[cur], array, [])\n",
        "\n",
        "print(\"중복 제거 전 : \", array)\n",
        "print(\"중복 제거 후 : \", result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msVBRiUylNmt",
        "outputId": "32be6829-6d27-4034-9f3b-2236727f9598"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "중복 제거 전 :  ['F', 'D', 'A', 'C', 'A', 'C', 'F', 'B', 'C', 'E', 'D', 'C', 'F', 'A', 'B', 'E', 'F', 'E']\n",
            "중복 제거 후 :  ['F', 'D', 'A', 'C', 'B', 'E']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 2, 'e': 1}\n",
        "\n",
        "seen = []\n",
        "result = dict()\n",
        "for key, val in my_dict.items():\n",
        "    if val not in seen:\n",
        "        seen.append(val)\n",
        "        result[key] = val\n",
        "\n",
        "print(f'Original dict: {my_dict}')\n",
        "print(f'Dict after removal: {result}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWfCR0JrqP43",
        "outputId": "7bb2ba37-1b45-49fc-f79f-6f3c47454322"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dict: {'a': 1, 'b': 2, 'c': 3, 'd': 2, 'e': 1}\n",
            "Dict after removal: {'a': 1, 'b': 2, 'c': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word embedding 실습"
      ],
      "metadata": {
        "id": "o3WFAYK8E1C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install feedparser\n",
        "#!pip install newspaper3k\n",
        "#!pip install konlpy\n",
        "\n",
        "#!pip install bs4\n",
        "#!sudo apt-get install -y fonts-nanum\n",
        "#!sudo fc-cache -fv\n",
        "#!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "import feedparser\n",
        "from newspaper import Article\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "from operator import eq\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "urls = [\"http://rss.etnews.com/Section901.xml\",\n",
        "        \"http://rss.etnews.com/Section902.xml\",\n",
        "        \"http://rss.etnews.com/Section903.xml\",\n",
        "        \"http://rss.etnews.com/Section904.xml\"]\n",
        "\n",
        "#[1]\n",
        "def crawl_rss(urls):\n",
        "  arr_rss = []\n",
        "  for url in urls:\n",
        "#->    print(\"[Crawl RSS]\", url)\n",
        "    parse_rss = feedparser.parse(url);\n",
        "    for p in parse_rss.entries:\n",
        "        arr_rss.append({'title':p.title, 'link':p.link})\n",
        "  return arr_rss\n",
        "\n",
        "#[3-1] url에서 기사를 읽어 온다.\n",
        "def crawl_article(url, language='ko'):\n",
        "#-> print(\"[Crawl Article]\",url)\n",
        "  var_article = Article(url, language=language)\n",
        "  var_article.download()\n",
        "  var_article.parse()\n",
        "  return var_article.title, var_article.text\n",
        "\n",
        "#[4-1] 읽어온 기사에서 키워드의 빈도를 측정한다.\n",
        "def get_keywords(text, nKeywords=50):\n",
        "  spliter = Okt()\n",
        "  nouns = spliter.nouns(text)\n",
        "  count = Counter(nouns)\n",
        "  return_list = []\n",
        "  for n, c in count.most_common(nKeywords):\n",
        "    temp = {'keyword':n, 'count': c}\n",
        "    return_list.append(temp)\n",
        "    #print(temp)\n",
        "  return return_list\n",
        "\n",
        "#[5-1] 입력된 단어를 키워드 리스트에서 검색하고 결과를 돌려 준다.  \n",
        "def Howmanywords(request, keyword):\n",
        "  nWords = 0\n",
        "  for n in keyword:\n",
        "    noun = n['keyword']\n",
        "    count = n['count']\n",
        "    if eq(noun, request):\n",
        "      nWords = count\n",
        "  return nWords\n",
        "\n",
        "#[2]\n",
        "article_list = crawl_rss(urls)\n",
        "print(article_list)\n",
        "#print(*article_list,sep='\\n')     #{title: ????, link: ????}\n",
        "\n",
        "'''\n",
        "#[3]\n",
        "# Part 2\n",
        "for article in article_list:\n",
        "  _, text = crawl_article(article['link'])\n",
        "  article['text'] = text\n",
        "#  print(text)\n",
        "\n",
        "\n",
        "#[4]\n",
        "# Part 3 : 수집된 텍스트 에서 키워드, 빈도를 리스트로 만든다.\n",
        "print('[Parsing Text]')\n",
        "for article in article_list:\n",
        "  keywords = get_keywords(article['text'])\n",
        "  \n",
        "  print(f'keywords: {keywords.type()}')\n",
        "  article['keywords'] = keywords\n",
        "\n",
        "print(f'article: {article}')\n",
        "print(f'Original dict:', len(article.keys()))\n",
        "'''\n",
        "\n",
        "'''\n",
        "seen = []\n",
        "result = dict()\n",
        "for key, val in article.items():\n",
        "    if val not in seen:\n",
        "        seen.append(val)\n",
        "        result[key] = val\n",
        "\n",
        "print(f'Dict after removal:', len(result.keys()))\n",
        "'''\n",
        "\n",
        "'''\n",
        "#[5]\n",
        "# Part 4: 입력된 키워드 검색\n",
        "print('Enter your query: ')\n",
        "query = input()\n",
        "for result in article_list:\n",
        "  n = Howmanywords(query, result['keywords'])\n",
        "  if n!=0:\n",
        "    print('[TF]:', n, '[Title]:', result['title'], '[URL]:', result['link'],'\\n')\n",
        "'''\n",
        "\n",
        "#[5]\n",
        "# Part 4: 입력된 키워드 검색\n",
        "#print('Enter your query: ')\n",
        "#query = input()\n",
        "#for article in article_list:\n",
        "#  n = Howmanywords(query, article['keywords'])\n",
        "#  if n!=0:\n",
        "#    print('[TF]:', n, '[Title]:', article['title'], '[URL]:', article['link'],'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "e3bbKt3-E-Im",
        "outputId": "e518afc0-8d1c-4b13-9f96-792915e1d790"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-2a32c29a83e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0marticle_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrawl_rss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;31m#print(*article_list,sep='\\n')     #{title: ????, link: ????}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'type'"
          ]
        }
      ]
    }
  ]
}